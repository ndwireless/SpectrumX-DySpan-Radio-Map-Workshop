{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecdbe95-b646-4376-a709-36e9c6076824",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "In this notebook, we will cover some basics of PyTorch that we will use in the rest of the tutorial.\n",
    "'''\n",
    "import torch\n",
    "import h5py\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abcc596-c6eb-44ff-8cc7-1b68bb716cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PyTorch Tensors\n",
    "PyTorch provides its own multi-dimensional matrix objects called Tensors. Tensors are similar to Numpy arrays in that they can be multi-dimensional, have\n",
    "many built-in functions and methods, and can only contain elements of a single data type. One of the biggest advantages of Tensors is that they have an \n",
    "Autograd functionality allowing for the computation of gradients for operations performed with the Tensor. We will provide a few examples of using Tensors.\n",
    "For more questions, we provide the link to PyTorch's documentation for Tensors: https://pytorch.org/docs/stable/tensors.html\n",
    "'''\n",
    "\n",
    "# Initializing Tensors - Deterministic: Tensors can be initialized from a list of elements or a Numpy array\n",
    "print(\"INITIALIZING TENSORS - DETERMINISTIC\")\n",
    "torch.set_default_dtype(torch.float32)\n",
    "data_list = [1.0, 2.0, 3.0]\n",
    "data_numpy = np.array(data_list)\n",
    "data_tensor1 = torch.tensor(data_list)\n",
    "data_tensor2 = torch.from_numpy(data_numpy)\n",
    "print(f\"We can print the resulting tensors to verify the elements match as expected. We can also see that the Tensor initialized from a Numpy array kept the array's dtype: float64.\")\n",
    "print(f\"Data Tensor 1: {data_tensor1}, Data Tensor 2: {data_tensor2}\")\n",
    "\n",
    "# Initializing Tensors - Random: PyTorch provides similar functions to Python and Numpy's 'random' modules for generating pseudo-random Tensors\n",
    "print()\n",
    "print(\"INITIALIZING TENSORS - RANDOM\")\n",
    "torch.manual_seed(0)\n",
    "random_tensor_dimensions = (2, 4, 2)\n",
    "random_tensor_normal = torch.randn(*random_tensor_dimensions)  # Generate random normal variables\n",
    "random_tensor_uniform_integers = torch.randint(low=0, high=9, size=random_tensor_dimensions)\n",
    "print(f\"Random normal tensor:\\n{random_tensor_normal}\")\n",
    "print(f\"Random uniform integers tensor:\\n{random_tensor_uniform_integers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28927c4-9cc5-4e02-a084-7b35088bad11",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Using PyTorch Tensors\n",
    "PyTorch and its Tensors have many helpful built-in functions & methods. We'll cover a handful of them here for future notebooks.\n",
    "'''\n",
    "# First, let's make some random data in a tensor to show off the functions and methods.\n",
    "data = torch.randn(3, 4)\n",
    "print(f'Data:\\n{data}')\n",
    "\n",
    "# We can get the dimensionality of a tensor using the '.shape' and '.size()' attribute and method. '.shape' returns all the dimensions while '.size()'\n",
    "# takes in a 'dim' argument to get the size of the dimension of interest.\n",
    "print()\n",
    "data_shape = data.shape\n",
    "data_rows = data.size(dim=0)\n",
    "data_cols = data.size(dim=1)\n",
    "print(f'Data Shape: {data_shape}, Size of Dim 0: {data_rows}, Size of Dim 1: {data_cols}')\n",
    "\n",
    "# Logical operators can be used on tensors to create a boolean tensor\n",
    "print()\n",
    "data_greater_than_zero = (data > 0)\n",
    "data_less_than_zero = (data < 0)\n",
    "print(f'Logical Tensors of values greater than 0:\\n{data_greater_than_zero}')\n",
    "\n",
    "# Means of tensors can easily be computed using the '.mean()' function. We will use this function later to compute the accuracy of our \n",
    "# occupancy estimates. By default, PyTorch uses the dtype of the provided tensor to know what dtype to keep the mean with. So, if we want to\n",
    "# compute the mean of a boolean tensor (the percentage of True's), we need to specify the dtype to use.\n",
    "print()\n",
    "data_mean = torch.mean(data)\n",
    "data_logical_mean = torch.mean(data_greater_than_zero, dtype=torch.float32)\n",
    "print(f'Data Mean: {data_mean}, Logical Mean: {data_logical_mean}')\n",
    "\n",
    "# The minimum and maximum value of a tensor can be found using PyTorch's built-in '.min()' and '.max()' functions.\n",
    "print()\n",
    "data_max = torch.max(data)\n",
    "data_min = torch.min(data)\n",
    "print(f'Minimum of the Data: {data_min}, Maximum of the Data: {data_max}')\n",
    "print(f'Type of data_min: {type(data_min)}')\n",
    "\n",
    "# There are times we'll want to convert our singular value tensors (like 'data_min' and 'data_max') to their respective Python data type equivalents.\n",
    "# We can use the '.item()' method to convert the singlular value tensor.\n",
    "print()\n",
    "data_max_python = data_max.item()\n",
    "data_min_python = data_min.item()\n",
    "print(f'Minimum of the Data: {data_min_python}, Maximum of the Data: {data_max_python}')\n",
    "print(f'Type of data_min_python: {type(data_min_python)}')\n",
    "\n",
    "# We can also sum the elements of a tensor using the '.sum()' function.\n",
    "print()\n",
    "data_sum = torch.sum(data)\n",
    "print(f'Data Sum: {data_sum}')\n",
    "\n",
    "# Many of PyTorch's tensor functions can be done only on certain dimensions including '.sum()', '.min()', and '.max()'. We'll show an example using\n",
    "# the '.sum()' function\n",
    "print()\n",
    "data_row_sums = torch.sum(data, dim=[1], keepdim=True)\n",
    "data_col_sums = torch.sum(data, dim=[0], keepdim=True)\n",
    "print(f'Data Row Sums:\\n{data_row_sums}\\nData Col Sums:\\n{data_col_sums}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8ba258-0a1b-4c8e-9497-3cc571e7e675",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "PyTorch Models\n",
    "PyTorch provides several useful classes of neural network layer components that can be concatenated for generating neural models.\n",
    "It is possible to train the neural networks using optimization techniques like gradient descent. However, training is beyond the scope of this notebook.\n",
    "Those interested in training are redirected to PyTorch's training tutorial: https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
    "'''\n",
    "# First, let's make some random data in a tensor\n",
    "batch_size = 8\n",
    "feature_size = 16\n",
    "data = torch.randn(batch_size, feature_size)\n",
    "\n",
    "# Now, let's make a simple feedforward network with two Linear layers and an activation function in between\n",
    "# Models: PyTorch allows the construction of models that can be trained, saved, and loaded. \n",
    "intermediate_size = 8\n",
    "output_size = 1\n",
    "ffn1 = torch.nn.Linear(feature_size, intermediate_size)\n",
    "act1 = torch.nn.ReLU()\n",
    "ffn2 = torch.nn.Linear(intermediate_size, output_size)\n",
    "model = torch.nn.Sequential(ffn1, act1, ffn2)\n",
    "model.eval()  # We need to put our model into '.eval()' mode for inference\n",
    "\n",
    "# Finally, let's test the model is functioning properly by feed it our test data. Models can be called like functions for inference.\n",
    "print(f'Data Shape: {data.shape}')\n",
    "output = model(data)\n",
    "print(f'Output Shape: {output.shape}, Expected Shape: ({batch_size}, {output_size})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72794b5f-82ad-4b58-9840-20d7c394977e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Plotting PyTorch Tensors with Matplotlib.pyplot\n",
    "In these notebooks, we will be plotting our produced radio maps using Matplotlib.pyplot using the imshow function.\n",
    "'''\n",
    "# First, let's plot some normal noise images. We'll also take advantage of '.imshow()'s arguments: 'interpolation', 'vmin', and 'vmax' to make sure\n",
    "# our images look nice. 'interpolation' allows us to control how \n",
    "data_noise = torch.randn(16, 16)\n",
    "data_noise_threshold = data_noise > 0\n",
    "plt.figure()\n",
    "fig, axs = plt.subplots(1, 2, figsize=(7, 3))\n",
    "fig.suptitle(\"Normal Noise Images\")\n",
    "axs[0].imshow(data_noise)\n",
    "axs[1].imshow(data_noise_threshold, interpolation='nearest', vmin=0, vmax=1) # We'll use 'nearest' interpolation so our binary image doesn't blend colors\n",
    "axs[0].axis('off')\n",
    "axs[1].axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Next, we'll plot Scipy's provided sample image\n",
    "image_colored = torch.tensor(scipy.datasets.face())\n",
    "image_gray = torch.tensor(scipy.datasets.face(gray=True))\n",
    "plt.figure()\n",
    "fig, axs = plt.subplots(1, 2, figsize=(7, 3))\n",
    "fig.suptitle(\"Masked Bandit\")\n",
    "axs[0].imshow(image_colored)\n",
    "axs[1].imshow(image_gray, cmap='gray') # We can also control the color by changing the 'cmap'. Other cmaps: https://matplotlib.org/stable/users/explain/colors/colormaps.html\n",
    "axs[0].axis('off')\n",
    "axs[1].axis('off')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283823ec-9367-4fb2-a864-0635419c6f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
